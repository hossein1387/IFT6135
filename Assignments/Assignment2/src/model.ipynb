{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT 6135 A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch.cuda\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets\n",
    "from os.path import dirname, abspath, join\n",
    "from torch import norm\n",
    "import config\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "cuda_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = dirname(dirname(abspath('__file__')))\n",
    "yaml_file = join(parent_dir, 'config.yaml')\n",
    "config = config.Configuration('Q1_1', yaml_file)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config):\n",
    "    mnist_train = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    mnist_test = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)\n",
    "    \n",
    "    test_sampler, valid_sampler = test_valid_split(mnist_test, config)\n",
    "    \n",
    "    trainloader = DataLoader(mnist_train, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = DataLoader(mnist_test, batch_size=64, sampler=test_sampler, num_workers=2)\n",
    "    validloader = DataLoader(mnist_test, batch_size=64, sampler=valid_sampler, num_workers=2)\n",
    "    return trainloader, testloader, validloader\n",
    "\n",
    "def test_valid_split(test, config):\n",
    "    num_test = len(test[0])\n",
    "    indices = list(range(num_test))\n",
    "    split = int(np.floor(num_test / 2))\n",
    "\n",
    "    # split test set into validation and test set\n",
    "    valid_idx, test_idx = indices[split:], indices[:split]\n",
    "\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "    return test_sampler, valid_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPb, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.Dropout(p=0.5), #last layer dropout\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10), \n",
    "            nn.Softmax(dim=0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLPa, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 600),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(600, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.Softmax(dim=0))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNa(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNa, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "        # Logistic Regression\n",
    "        self.clf = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.clf(self.conv(x).squeeze())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNb, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=(3, 3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=2)\n",
    "        )\n",
    "        # Logistic Regression\n",
    "        self.clf = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.clf(self.conv(x).squeeze())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if config.model_type == 'MLPa':\n",
    "        model = MLPa()\n",
    "    elif config.model_type == 'MLPb':\n",
    "        model = MLPb()\n",
    "    elif config.model_type == 'CNN':\n",
    "        if not config.batch_norm:\n",
    "            model = CNNa()\n",
    "        else:\n",
    "            model = CNNb()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "         \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=config.lr0, weight_decay = config.weight_decay/config.batch_size)\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES TENSORBOARDX http://tensorboard-pytorch.readthedocs.io/en/latest/tensorboard.html\n",
    "\n",
    "def train_model(config, model, criterion, optimizer):\n",
    "    losses = []\n",
    "    parameter_norms = []\n",
    "    writer = SummaryWriter('isaacsultan/IFT6135/Assignments/Assignment2/logs')\n",
    "    # record the performance for this epoch\n",
    "    trainloader, testloader, validloader = load_dataset(config)\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "            parameter_norm = []\n",
    "            optimizer.zero_grad()\n",
    "             \n",
    "            if config.model_type == 'MLPa' or config.model_type == 'MLPb':\n",
    "                inputs = Variable(inputs).view(-1,784)\n",
    "                targets = Variable(targets).view(-1)\n",
    "            elif model_type == 'CNN':\n",
    "                inputs = Variable(inputs).view(-1,1,28,28)\n",
    "                targets = Variable(y).view(-1)\n",
    "            if cuda_available:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # compute loss\n",
    "            loss = criterion(model(inputs), targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.data[0])\n",
    "            test_loss = find_testloss(model, testloader)\n",
    "            writer.add_scalars('learning_curve', {'train_loss':loss.data[0], 'test_loss':test_loss}, batch_idx)\n",
    "            for param in model.parameters():\n",
    "                parameter_norm.append(norm(param))\n",
    "        # print the results for this epoch\n",
    "        print(\"Epoch {0} \\n Train Loss : {1:.3f} \\Test Loss : {2:.3f}\".format(epoch, np.mean(losses), test_loss))\n",
    "        parameter_norms.append(parameter_norm) #and dump "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_testloss(model,test_loader):\n",
    "    model.eval() #import when using dropout\n",
    "    test_loss_iter = []\n",
    "    for data in test_loader:\n",
    "        inputs, targets = data\n",
    "\n",
    "        if config.model_type == 'MLPa' or config.model_type == 'MLPb':\n",
    "            inputs = Variable(inputs, volatile=True).view(-1,784)\n",
    "            targets = Variable(targets, volatile=True).view(-1)\n",
    "        elif model_type == 'CNN':\n",
    "            inputs = Variable(inputs, volatile=True).view(-1,1,28,28)\n",
    "            targets = Variable(targets, volatile=True).view(-1)\n",
    "        if cuda_available:\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        test_loss = criterion(outputs, targets)\n",
    "        test_loss_iter.append(test_loss)\n",
    "    iteration_test_loss = np.mean(test_loss_iter)\n",
    "    return iteration_test_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = build_model()\n",
    "train_model(config, model, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
